{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[🧠🧬 EDA+3D-Baseline – RSNA – Glioma Radiogenomics](https://www.kaggle.com/dschettler8845/eda-3d-baseline-rsna-glioma-radiogenomics/notebook#modelling) の\n",
    "`6.2 CREATE THE DATASETS` 部分を解読。`train_edited.csv` は以下手順で作成。\n",
    "\n",
    "```python\n",
    "ROOT_DIR = '../input/rsna-miccai-brain-tumor-radiogenomic-classification'\n",
    "TRAIN_DIR = os.path.join(ROOT_DIR, 'train')\n",
    "TRAIN_CSV = os.path.join(ROOT_DIR, 'train_labels.csv')\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "TRAIN_NPY_DIR = \"/kaggle/input/create-3d-npz-rsna-radiogenomic-classification/test\"\n",
    "TRAIN_FLAIR_NPY_DIR = os.path.join(TRAIN_NPY_DIR, \"FLAIR\")\n",
    "train_df[\"path_to_flair_dir\"] = train_df.BraTS21ID.apply(lambda x: os.path.join(TRAIN_DIR, f\"{x:>05}\", \"FLAIR\"))\n",
    "train_df[\"flair_image_count\"] = train_df.path_to_flair_dir.apply(lambda x: len(os.listdir(x)))\n",
    "train_df[\"path_to_flair_npz\"] = train_df.BraTS21ID.apply(lambda x: os.path.join(TRAIN_FLAIR_NPY_DIR, f\"{x:>05}.npz\"))\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### まとめ\n",
    "\n",
    "評価用データ、テスト用データの処理も含まれているので煩雑に見える。  \n",
    "訓練データの部分を抜き出すと以下のようになる。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_npz(np_file_path, is_tf=False):\n",
    "    if is_tf:\n",
    "        return np.load(np_file_path.numpy().decode())[\"arr_0\"] \n",
    "    else:\n",
    "        return np.load(np_file_path)[\"arr_0\"] \n",
    "\n",
    "INPUT_SHAPE = (128,128,32,1)\n",
    "BATCH_SIZE = 8\n",
    "train_df = pd.read_csv(\"train_edited.csv\")\n",
    "dataset_storage = {}\n",
    "use_col = \"path_to_flair_npz\"\n",
    "col_identifier = use_col.split(\"_\")[2] # flair\n",
    "dataset_storage[col_identifier] = {}\n",
    "\n",
    "# npz ファイルとその MGMT_value を紐付け\n",
    "dataset_storage[col_identifier][\"train_ds_df\"] = train_df[train_df[use_col].apply(lambda x: os.path.isfile(x))][[\"MGMT_value\", use_col]]\n",
    "# 上記 MGMT_value をリスト化 (lbl = label)\n",
    "dataset_storage[col_identifier][\"train_lbl_list\"] = dataset_storage[col_identifier][\"train_ds_df\"].MGMT_value.to_list()\n",
    "# 上記 \"path_to_flair_npz\" をリスト化\n",
    "dataset_storage[col_identifier][\"train_npz_file_list\"] = dataset_storage[col_identifier][\"train_ds_df\"][use_col].to_list()\n",
    "\n",
    "# npz ファイルの数を算出\n",
    "dataset_storage[col_identifier][\"N_EX\"] = len(dataset_storage[col_identifier][\"train_lbl_list\"])\n",
    "dataset_storage[col_identifier][\"VAL_FRAC\"] = 0.1\n",
    "\n",
    "# npz ファイルの数 * (1 - 0.9) (訓練用データ)\n",
    "dataset_storage[col_identifier][\"N_TRAIN\"] = int(dataset_storage[col_identifier][\"N_EX\"]*(1-dataset_storage[col_identifier][\"VAL_FRAC\"]))\n",
    "\n",
    "# 重複なしで N_EX 個のサンプルをシャッフルされた状態で抽出\n",
    "dataset_storage[col_identifier][\"RANDOM_INDICES\"] = random.sample(range(dataset_storage[col_identifier][\"N_EX\"]), dataset_storage[col_identifier][\"N_EX\"])\n",
    "# ↑から N_EX * 0.9 個抽出\n",
    "dataset_storage[col_identifier][\"TRAIN_INDICES\"] = dataset_storage[col_identifier][\"RANDOM_INDICES\"][:dataset_storage[col_identifier][\"N_TRAIN\"]]\n",
    "# 訓練用データのラベルを tf.data.Dataset 化\n",
    "dataset_storage[col_identifier][\"lbl_train_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_lbl_list\"])[dataset_storage[col_identifier][\"TRAIN_INDICES\"]])\n",
    "# 訓練用データの npz ファイルへのパスを tf.data.Dataset 化\n",
    "dataset_storage[col_identifier][\"npz_file_train_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_npz_file_list\"])[dataset_storage[col_identifier][\"TRAIN_INDICES\"]])\n",
    "# 訓練用データの npz ファイルへのパスとラベルを zip 化\n",
    "dataset_storage[col_identifier][\"raw_train_ds\"] = tf.data.Dataset.zip((dataset_storage[col_identifier][\"npz_file_train_ds\"], dataset_storage[col_identifier][\"lbl_train_ds\"]))\n",
    "\n",
    "# 1. npz ファイルから画像データをロード(tf.py_function(load_npz, [x, True], [tf.unit8]))\n",
    "# 2. (128, 128, 32, 1) に形状を変化 (tf.reshape)\n",
    "# 3. 正規化 ( /255)\n",
    "# 4. shuffle -> batch -> prefetch(GPUに最適化)\n",
    "dataset_storage[col_identifier][\"train_ds\"] = dataset_storage[col_identifier][\"raw_train_ds\"].map(lambda x,y: (\n",
    "    tf.reshape(\n",
    "        tf.py_function(\n",
    "            load_npz, [x, True], (tf.uint8,)\n",
    "        ),\n",
    "        INPUT_SHAPE\n",
    "    )/255,\n",
    "    tf.cast(y, tf.uint8)\n",
    "), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset_storage[col_identifier][\"train_ds\"] = dataset_storage[col_identifier][\"train_ds\"].shuffle(BATCH_SIZE*5).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-05 10:38:12.477650: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 解読用"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_df = pd.read_csv(\"train_edited.csv\")\n",
    "\n",
    "dataset_storage = {}\n",
    "use_col = \"path_to_flair_npz\"\n",
    "col_identifier = use_col.split(\"_\")[2] # flair\n",
    "dataset_storage[col_identifier] = {}\n",
    "\n",
    "# npz ファイルとその MGMT_value を紐付け\n",
    "dataset_storage[col_identifier][\"train_ds_df\"] = train_df[train_df[use_col].apply(lambda x: os.path.isfile(x))][[\"MGMT_value\", use_col]]\n",
    "# 上記 MGMT_value をリスト化 (lbl = label)\n",
    "dataset_storage[col_identifier][\"train_lbl_list\"] = dataset_storage[col_identifier][\"train_ds_df\"].MGMT_value.to_list()\n",
    "# 上記 \"path_to_flair_npz\" をリスト化\n",
    "dataset_storage[col_identifier][\"train_npz_file_list\"] = dataset_storage[col_identifier][\"train_ds_df\"][use_col].to_list()\n",
    "\n",
    "# ↑と同じことを test ディレクトリにも適用\n",
    "# dataset_storage[col_identifier][\"test_ds_df\"] = ss_df[ss_df[use_col].apply(lambda x: True if os.path.isfile(x) else False)][[\"BraTS21ID\", use_col]]\n",
    "# dataset_storage[col_identifier][\"test_id_list\"] = dataset_storage[col_identifier][\"test_ds_df\"].BraTS21ID.to_list()\n",
    "# dataset_storage[col_identifier][\"test_npz_file_list\"] = dataset_storage[col_identifier][\"test_ds_df\"][use_col].to_list()\n",
    "\n",
    "# This is for splitting\n",
    "\n",
    "# npz ファイルの数\n",
    "dataset_storage[col_identifier][\"N_EX\"] = len(dataset_storage[col_identifier][\"train_lbl_list\"])\n",
    "dataset_storage[col_identifier][\"VAL_FRAC\"] = 0.1\n",
    "\n",
    "# npz ファイルの数 * (1 - 0.9) (訓練用データ)\n",
    "dataset_storage[col_identifier][\"N_TRAIN\"] = int(dataset_storage[col_identifier][\"N_EX\"]*(1-dataset_storage[col_identifier][\"VAL_FRAC\"]))\n",
    "# npz ファイルの数 * 0.1 (バリデーション用データ)\n",
    "# dataset_storage[col_identifier][\"N_VAL\"] = dataset_storage[col_identifier][\"N_EX\"]-dataset_storage[col_identifier][\"N_TRAIN\"]\n",
    "\n",
    "# 重複なしで N_EX 個のサンプルをシャッフルされた状態で抽出\n",
    "dataset_storage[col_identifier][\"RANDOM_INDICES\"] = random.sample(range(dataset_storage[col_identifier][\"N_EX\"]), dataset_storage[col_identifier][\"N_EX\"])\n",
    "# ↑から N_EX * 0.9 個抽出\n",
    "dataset_storage[col_identifier][\"TRAIN_INDICES\"] = dataset_storage[col_identifier][\"RANDOM_INDICES\"][:dataset_storage[col_identifier][\"N_TRAIN\"]]\n",
    "# ↑から N_EX * 0.1 個抽出\n",
    "# dataset_storage[col_identifier][\"VAL_INDICES\"] = dataset_storage[col_identifier][\"RANDOM_INDICES\"][dataset_storage[col_identifier][\"N_TRAIN\"]:]\n",
    "\n",
    "pprint.pprint(dataset_storage)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'flair': {'N_EX': 0,\n",
      "           'N_TRAIN': 0,\n",
      "           'RANDOM_INDICES': [],\n",
      "           'TRAIN_INDICES': [],\n",
      "           'VAL_FRAC': 0.1,\n",
      "           'train_ds_df': Empty DataFrame\n",
      "Columns: [MGMT_value, path_to_flair_npz]\n",
      "Index: [],\n",
      "           'train_lbl_list': [],\n",
      "           'train_npz_file_list': []}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 訓練用データのラベルを tf.data.Dataset 化\n",
    "dataset_storage[col_identifier][\"lbl_train_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_lbl_list\"])[dataset_storage[col_identifier][\"TRAIN_INDICES\"]])\n",
    "# 訓練用データの npz ファイルへのパスを tf.data.Dataset 化\n",
    "dataset_storage[col_identifier][\"npz_file_train_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_npz_file_list\"])[dataset_storage[col_identifier][\"TRAIN_INDICES\"]])\n",
    "# 訓練用データの npz ファイルへのパスとラベルを zip 化\n",
    "dataset_storage[col_identifier][\"raw_train_ds\"] = tf.data.Dataset.zip((dataset_storage[col_identifier][\"npz_file_train_ds\"], dataset_storage[col_identifier][\"lbl_train_ds\"]))\n",
    "\n",
    "# バリデーションデータで上記と同じことを実施\n",
    "# dataset_storage[col_identifier][\"lbl_val_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_lbl_list\"])[dataset_storage[col_identifier][\"VAL_INDICES\"]])\n",
    "# dataset_storage[col_identifier][\"npz_file_val_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_npz_file_list\"])[dataset_storage[col_identifier][\"VAL_INDICES\"]])\n",
    "# dataset_storage[col_identifier][\"raw_val_ds\"] = tf.data.Dataset.zip((dataset_storage[col_identifier][\"npz_file_val_ds\"], dataset_storage[col_identifier][\"lbl_val_ds\"]))\n",
    "\n",
    "# テストデータで上記と同じことを実施\n",
    "# dataset_storage[col_identifier][\"id_test_ds\"] = tf.data.Dataset.from_tensor_slices(dataset_storage[col_identifier][\"test_id_list\"])\n",
    "# dataset_storage[col_identifier][\"npz_file_test_ds\"] = tf.data.Dataset.from_tensor_slices(dataset_storage[col_identifier][\"test_npz_file_list\"])\n",
    "# dataset_storage[col_identifier][\"raw_test_ds\"] = tf.data.Dataset.zip((dataset_storage[col_identifier][\"npz_file_test_ds\"], dataset_storage[col_identifier][\"id_test_ds\"]))\n",
    "\n",
    "def load_npz(np_file_path, is_tf=False):\n",
    "    if is_tf:\n",
    "        return np.load(np_file_path.numpy().decode())[\"arr_0\"] \n",
    "    else:\n",
    "        return np.load(np_file_path)[\"arr_0\"] \n",
    "\n",
    "INPUT_SHAPE = (128,128,32,1)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# 1. npz ファイルから画像データをロード(tf.py_function(load_npz, [x, True], [tf.unit8]))\n",
    "# 2. (128, 128, 32, 1) に形状を変化 (tf.reshape)\n",
    "# 3. 正規化 ( /255)\n",
    "# 4. shuffle -> batch -> prefetch(GPUに最適化)\n",
    "dataset_storage[col_identifier][\"train_ds\"] = dataset_storage[col_identifier][\"raw_train_ds\"].map(lambda x,y: (\n",
    "    tf.reshape(\n",
    "        tf.py_function(\n",
    "            load_npz, [x, True], (tf.uint8,)\n",
    "        ),\n",
    "        INPUT_SHAPE\n",
    "    )/255,\n",
    "    tf.cast(y, tf.uint8)\n",
    "), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset_storage[col_identifier][\"train_ds\"] = dataset_storage[col_identifier][\"train_ds\"].shuffle(BATCH_SIZE*5).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# dataset_storage[col_identifier][\"val_ds\"] = dataset_storage[col_identifier][\"raw_val_ds\"].map(lambda x,y: (tf.reshape(tf.py_function(\n",
    "#     load_npz, [x, True], (tf.uint8,)\n",
    "# ), INPUT_SHAPE)/255, tf.cast(y, tf.uint8)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# dataset_storage[col_identifier][\"val_ds\"] = dataset_storage[col_identifier][\"val_ds\"].batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# dataset_storage[col_identifier][\"test_ds\"] = dataset_storage[col_identifier][\"raw_test_ds\"].map(lambda x,y: (tf.reshape(tf.py_function(\n",
    "#     load_npz, [x, True], (tf.uint8,)\n",
    "# ), INPUT_SHAPE)/255, tf.cast(y, tf.int32)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# dataset_storage[col_identifier][\"test_ds\"] = dataset_storage[col_identifier][\"test_ds\"].batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "INPUT_SHAPE = (128,128,32,1)\n",
    "INPUT_SHAPE[-1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('3.9.7': pyenv)"
  },
  "interpreter": {
   "hash": "547c98e2d128f0d954a25014c53f40094ed93b4078667c378730fcf6748ca4ce"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}