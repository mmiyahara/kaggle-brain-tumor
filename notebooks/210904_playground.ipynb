{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[üß†üß¨ EDA+3D-Baseline ‚Äì RSNA ‚Äì Glioma Radiogenomics](https://www.kaggle.com/dschettler8845/eda-3d-baseline-rsna-glioma-radiogenomics/notebook#modelling) „ÅÆ\n",
    "`6.2 CREATE THE DATASETS` ÈÉ®ÂàÜ„ÇíËß£Ë™≠„ÄÇ`train_edited.csv` „ÅØ‰ª•‰∏ãÊâãÈ†Ü„Åß‰ΩúÊàê„ÄÇ\n",
    "\n",
    "```python\n",
    "ROOT_DIR = '../input/rsna-miccai-brain-tumor-radiogenomic-classification'\n",
    "TRAIN_DIR = os.path.join(ROOT_DIR, 'train')\n",
    "TRAIN_CSV = os.path.join(ROOT_DIR, 'train_labels.csv')\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "TRAIN_NPY_DIR = \"/kaggle/input/create-3d-npz-rsna-radiogenomic-classification/test\"\n",
    "TRAIN_FLAIR_NPY_DIR = os.path.join(TRAIN_NPY_DIR, \"FLAIR\")\n",
    "train_df[\"path_to_flair_dir\"] = train_df.BraTS21ID.apply(lambda x: os.path.join(TRAIN_DIR, f\"{x:>05}\", \"FLAIR\"))\n",
    "train_df[\"flair_image_count\"] = train_df.path_to_flair_dir.apply(lambda x: len(os.listdir(x)))\n",
    "train_df[\"path_to_flair_npz\"] = train_df.BraTS21ID.apply(lambda x: os.path.join(TRAIN_FLAIR_NPY_DIR, f\"{x:>05}.npz\"))\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### „Åæ„Å®„ÇÅ\n",
    "\n",
    "Ë©ï‰æ°Áî®„Éá„Éº„Çø„ÄÅ„ÉÜ„Çπ„ÉàÁî®„Éá„Éº„Çø„ÅÆÂá¶ÁêÜ„ÇÇÂê´„Åæ„Çå„Å¶„ÅÑ„Çã„ÅÆ„ÅßÁÖ©Èõë„Å´Ë¶ã„Åà„Çã„ÄÇ  \n",
    "Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆÈÉ®ÂàÜ„ÇíÊäú„ÅçÂá∫„Åô„Å®‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´„Å™„Çã„ÄÇ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_npz(np_file_path, is_tf=False):\n",
    "    if is_tf:\n",
    "        return np.load(np_file_path.numpy().decode())[\"arr_0\"] \n",
    "    else:\n",
    "        return np.load(np_file_path)[\"arr_0\"] \n",
    "\n",
    "INPUT_SHAPE = (128,128,32,1)\n",
    "BATCH_SIZE = 8\n",
    "train_df = pd.read_csv(\"train_edited.csv\")\n",
    "dataset_storage = {}\n",
    "use_col = \"path_to_flair_npz\"\n",
    "col_identifier = use_col.split(\"_\")[2] # flair\n",
    "dataset_storage[col_identifier] = {}\n",
    "\n",
    "# npz „Éï„Ç°„Ç§„É´„Å®„Åù„ÅÆ MGMT_value „ÇíÁ¥ê‰ªò„Åë\n",
    "dataset_storage[col_identifier][\"train_ds_df\"] = train_df[train_df[use_col].apply(lambda x: os.path.isfile(x))][[\"MGMT_value\", use_col]]\n",
    "# ‰∏äË®ò MGMT_value „Çí„É™„Çπ„ÉàÂåñ (lbl = label)\n",
    "dataset_storage[col_identifier][\"train_lbl_list\"] = dataset_storage[col_identifier][\"train_ds_df\"].MGMT_value.to_list()\n",
    "# ‰∏äË®ò \"path_to_flair_npz\" „Çí„É™„Çπ„ÉàÂåñ\n",
    "dataset_storage[col_identifier][\"train_npz_file_list\"] = dataset_storage[col_identifier][\"train_ds_df\"][use_col].to_list()\n",
    "\n",
    "# npz „Éï„Ç°„Ç§„É´„ÅÆÊï∞„ÇíÁÆóÂá∫\n",
    "dataset_storage[col_identifier][\"N_EX\"] = len(dataset_storage[col_identifier][\"train_lbl_list\"])\n",
    "dataset_storage[col_identifier][\"VAL_FRAC\"] = 0.1\n",
    "\n",
    "# npz „Éï„Ç°„Ç§„É´„ÅÆÊï∞ * (1 - 0.9) (Ë®ìÁ∑¥Áî®„Éá„Éº„Çø)\n",
    "dataset_storage[col_identifier][\"N_TRAIN\"] = int(dataset_storage[col_identifier][\"N_EX\"]*(1-dataset_storage[col_identifier][\"VAL_FRAC\"]))\n",
    "\n",
    "# ÈáçË§á„Å™„Åó„Åß N_EX ÂÄã„ÅÆ„Çµ„É≥„Éó„É´„Çí„Ç∑„É£„ÉÉ„Éï„É´„Åï„Çå„ÅüÁä∂ÊÖã„ÅßÊäΩÂá∫\n",
    "dataset_storage[col_identifier][\"RANDOM_INDICES\"] = random.sample(range(dataset_storage[col_identifier][\"N_EX\"]), dataset_storage[col_identifier][\"N_EX\"])\n",
    "# ‚Üë„Åã„Çâ N_EX * 0.9 ÂÄãÊäΩÂá∫\n",
    "dataset_storage[col_identifier][\"TRAIN_INDICES\"] = dataset_storage[col_identifier][\"RANDOM_INDICES\"][:dataset_storage[col_identifier][\"N_TRAIN\"]]\n",
    "# Ë®ìÁ∑¥Áî®„Éá„Éº„Çø„ÅÆ„É©„Éô„É´„Çí tf.data.Dataset Âåñ\n",
    "dataset_storage[col_identifier][\"lbl_train_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_lbl_list\"])[dataset_storage[col_identifier][\"TRAIN_INDICES\"]])\n",
    "# Ë®ìÁ∑¥Áî®„Éá„Éº„Çø„ÅÆ npz „Éï„Ç°„Ç§„É´„Å∏„ÅÆ„Éë„Çπ„Çí tf.data.Dataset Âåñ\n",
    "dataset_storage[col_identifier][\"npz_file_train_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_npz_file_list\"])[dataset_storage[col_identifier][\"TRAIN_INDICES\"]])\n",
    "# Ë®ìÁ∑¥Áî®„Éá„Éº„Çø„ÅÆ npz „Éï„Ç°„Ç§„É´„Å∏„ÅÆ„Éë„Çπ„Å®„É©„Éô„É´„Çí zip Âåñ\n",
    "dataset_storage[col_identifier][\"raw_train_ds\"] = tf.data.Dataset.zip((dataset_storage[col_identifier][\"npz_file_train_ds\"], dataset_storage[col_identifier][\"lbl_train_ds\"]))\n",
    "\n",
    "# 1. npz „Éï„Ç°„Ç§„É´„Åã„ÇâÁîªÂÉè„Éá„Éº„Çø„Çí„É≠„Éº„Éâ(tf.py_function(load_npz, [x, True], [tf.unit8]))\n",
    "# 2. (128, 128, 32, 1) „Å´ÂΩ¢Áä∂„ÇíÂ§âÂåñ (tf.reshape)\n",
    "# 3. Ê≠£Ë¶èÂåñ ( /255)\n",
    "# 4. shuffle -> batch -> prefetch(GPU„Å´ÊúÄÈÅ©Âåñ)\n",
    "dataset_storage[col_identifier][\"train_ds\"] = dataset_storage[col_identifier][\"raw_train_ds\"].map(lambda x,y: (\n",
    "    tf.reshape(\n",
    "        tf.py_function(\n",
    "            load_npz, [x, True], (tf.uint8,)\n",
    "        ),\n",
    "        INPUT_SHAPE\n",
    "    )/255,\n",
    "    tf.cast(y, tf.uint8)\n",
    "), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset_storage[col_identifier][\"train_ds\"] = dataset_storage[col_identifier][\"train_ds\"].shuffle(BATCH_SIZE*5).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-05 10:38:12.477650: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ëß£Ë™≠Áî®"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_df = pd.read_csv(\"train_edited.csv\")\n",
    "\n",
    "dataset_storage = {}\n",
    "use_col = \"path_to_flair_npz\"\n",
    "col_identifier = use_col.split(\"_\")[2] # flair\n",
    "dataset_storage[col_identifier] = {}\n",
    "\n",
    "# npz „Éï„Ç°„Ç§„É´„Å®„Åù„ÅÆ MGMT_value „ÇíÁ¥ê‰ªò„Åë\n",
    "dataset_storage[col_identifier][\"train_ds_df\"] = train_df[train_df[use_col].apply(lambda x: os.path.isfile(x))][[\"MGMT_value\", use_col]]\n",
    "# ‰∏äË®ò MGMT_value „Çí„É™„Çπ„ÉàÂåñ (lbl = label)\n",
    "dataset_storage[col_identifier][\"train_lbl_list\"] = dataset_storage[col_identifier][\"train_ds_df\"].MGMT_value.to_list()\n",
    "# ‰∏äË®ò \"path_to_flair_npz\" „Çí„É™„Çπ„ÉàÂåñ\n",
    "dataset_storage[col_identifier][\"train_npz_file_list\"] = dataset_storage[col_identifier][\"train_ds_df\"][use_col].to_list()\n",
    "\n",
    "# ‚Üë„Å®Âêå„Åò„Åì„Å®„Çí test „Éá„Ç£„É¨„ÇØ„Éà„É™„Å´„ÇÇÈÅ©Áî®\n",
    "# dataset_storage[col_identifier][\"test_ds_df\"] = ss_df[ss_df[use_col].apply(lambda x: True if os.path.isfile(x) else False)][[\"BraTS21ID\", use_col]]\n",
    "# dataset_storage[col_identifier][\"test_id_list\"] = dataset_storage[col_identifier][\"test_ds_df\"].BraTS21ID.to_list()\n",
    "# dataset_storage[col_identifier][\"test_npz_file_list\"] = dataset_storage[col_identifier][\"test_ds_df\"][use_col].to_list()\n",
    "\n",
    "# This is for splitting\n",
    "\n",
    "# npz „Éï„Ç°„Ç§„É´„ÅÆÊï∞\n",
    "dataset_storage[col_identifier][\"N_EX\"] = len(dataset_storage[col_identifier][\"train_lbl_list\"])\n",
    "dataset_storage[col_identifier][\"VAL_FRAC\"] = 0.1\n",
    "\n",
    "# npz „Éï„Ç°„Ç§„É´„ÅÆÊï∞ * (1 - 0.9) (Ë®ìÁ∑¥Áî®„Éá„Éº„Çø)\n",
    "dataset_storage[col_identifier][\"N_TRAIN\"] = int(dataset_storage[col_identifier][\"N_EX\"]*(1-dataset_storage[col_identifier][\"VAL_FRAC\"]))\n",
    "# npz „Éï„Ç°„Ç§„É´„ÅÆÊï∞ * 0.1 („Éê„É™„Éá„Éº„Ç∑„Éß„É≥Áî®„Éá„Éº„Çø)\n",
    "# dataset_storage[col_identifier][\"N_VAL\"] = dataset_storage[col_identifier][\"N_EX\"]-dataset_storage[col_identifier][\"N_TRAIN\"]\n",
    "\n",
    "# ÈáçË§á„Å™„Åó„Åß N_EX ÂÄã„ÅÆ„Çµ„É≥„Éó„É´„Çí„Ç∑„É£„ÉÉ„Éï„É´„Åï„Çå„ÅüÁä∂ÊÖã„ÅßÊäΩÂá∫\n",
    "dataset_storage[col_identifier][\"RANDOM_INDICES\"] = random.sample(range(dataset_storage[col_identifier][\"N_EX\"]), dataset_storage[col_identifier][\"N_EX\"])\n",
    "# ‚Üë„Åã„Çâ N_EX * 0.9 ÂÄãÊäΩÂá∫\n",
    "dataset_storage[col_identifier][\"TRAIN_INDICES\"] = dataset_storage[col_identifier][\"RANDOM_INDICES\"][:dataset_storage[col_identifier][\"N_TRAIN\"]]\n",
    "# ‚Üë„Åã„Çâ N_EX * 0.1 ÂÄãÊäΩÂá∫\n",
    "# dataset_storage[col_identifier][\"VAL_INDICES\"] = dataset_storage[col_identifier][\"RANDOM_INDICES\"][dataset_storage[col_identifier][\"N_TRAIN\"]:]\n",
    "\n",
    "pprint.pprint(dataset_storage)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'flair': {'N_EX': 0,\n",
      "           'N_TRAIN': 0,\n",
      "           'RANDOM_INDICES': [],\n",
      "           'TRAIN_INDICES': [],\n",
      "           'VAL_FRAC': 0.1,\n",
      "           'train_ds_df': Empty DataFrame\n",
      "Columns: [MGMT_value, path_to_flair_npz]\n",
      "Index: [],\n",
      "           'train_lbl_list': [],\n",
      "           'train_npz_file_list': []}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Ë®ìÁ∑¥Áî®„Éá„Éº„Çø„ÅÆ„É©„Éô„É´„Çí tf.data.Dataset Âåñ\n",
    "dataset_storage[col_identifier][\"lbl_train_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_lbl_list\"])[dataset_storage[col_identifier][\"TRAIN_INDICES\"]])\n",
    "# Ë®ìÁ∑¥Áî®„Éá„Éº„Çø„ÅÆ npz „Éï„Ç°„Ç§„É´„Å∏„ÅÆ„Éë„Çπ„Çí tf.data.Dataset Âåñ\n",
    "dataset_storage[col_identifier][\"npz_file_train_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_npz_file_list\"])[dataset_storage[col_identifier][\"TRAIN_INDICES\"]])\n",
    "# Ë®ìÁ∑¥Áî®„Éá„Éº„Çø„ÅÆ npz „Éï„Ç°„Ç§„É´„Å∏„ÅÆ„Éë„Çπ„Å®„É©„Éô„É´„Çí zip Âåñ\n",
    "dataset_storage[col_identifier][\"raw_train_ds\"] = tf.data.Dataset.zip((dataset_storage[col_identifier][\"npz_file_train_ds\"], dataset_storage[col_identifier][\"lbl_train_ds\"]))\n",
    "\n",
    "# „Éê„É™„Éá„Éº„Ç∑„Éß„É≥„Éá„Éº„Çø„Åß‰∏äË®ò„Å®Âêå„Åò„Åì„Å®„ÇíÂÆüÊñΩ\n",
    "# dataset_storage[col_identifier][\"lbl_val_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_lbl_list\"])[dataset_storage[col_identifier][\"VAL_INDICES\"]])\n",
    "# dataset_storage[col_identifier][\"npz_file_val_ds\"] = tf.data.Dataset.from_tensor_slices(np.array(dataset_storage[col_identifier][\"train_npz_file_list\"])[dataset_storage[col_identifier][\"VAL_INDICES\"]])\n",
    "# dataset_storage[col_identifier][\"raw_val_ds\"] = tf.data.Dataset.zip((dataset_storage[col_identifier][\"npz_file_val_ds\"], dataset_storage[col_identifier][\"lbl_val_ds\"]))\n",
    "\n",
    "# „ÉÜ„Çπ„Éà„Éá„Éº„Çø„Åß‰∏äË®ò„Å®Âêå„Åò„Åì„Å®„ÇíÂÆüÊñΩ\n",
    "# dataset_storage[col_identifier][\"id_test_ds\"] = tf.data.Dataset.from_tensor_slices(dataset_storage[col_identifier][\"test_id_list\"])\n",
    "# dataset_storage[col_identifier][\"npz_file_test_ds\"] = tf.data.Dataset.from_tensor_slices(dataset_storage[col_identifier][\"test_npz_file_list\"])\n",
    "# dataset_storage[col_identifier][\"raw_test_ds\"] = tf.data.Dataset.zip((dataset_storage[col_identifier][\"npz_file_test_ds\"], dataset_storage[col_identifier][\"id_test_ds\"]))\n",
    "\n",
    "def load_npz(np_file_path, is_tf=False):\n",
    "    if is_tf:\n",
    "        return np.load(np_file_path.numpy().decode())[\"arr_0\"] \n",
    "    else:\n",
    "        return np.load(np_file_path)[\"arr_0\"] \n",
    "\n",
    "INPUT_SHAPE = (128,128,32,1)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# 1. npz „Éï„Ç°„Ç§„É´„Åã„ÇâÁîªÂÉè„Éá„Éº„Çø„Çí„É≠„Éº„Éâ(tf.py_function(load_npz, [x, True], [tf.unit8]))\n",
    "# 2. (128, 128, 32, 1) „Å´ÂΩ¢Áä∂„ÇíÂ§âÂåñ (tf.reshape)\n",
    "# 3. Ê≠£Ë¶èÂåñ ( /255)\n",
    "# 4. shuffle -> batch -> prefetch(GPU„Å´ÊúÄÈÅ©Âåñ)\n",
    "dataset_storage[col_identifier][\"train_ds\"] = dataset_storage[col_identifier][\"raw_train_ds\"].map(lambda x,y: (\n",
    "    tf.reshape(\n",
    "        tf.py_function(\n",
    "            load_npz, [x, True], (tf.uint8,)\n",
    "        ),\n",
    "        INPUT_SHAPE\n",
    "    )/255,\n",
    "    tf.cast(y, tf.uint8)\n",
    "), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset_storage[col_identifier][\"train_ds\"] = dataset_storage[col_identifier][\"train_ds\"].shuffle(BATCH_SIZE*5).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# dataset_storage[col_identifier][\"val_ds\"] = dataset_storage[col_identifier][\"raw_val_ds\"].map(lambda x,y: (tf.reshape(tf.py_function(\n",
    "#     load_npz, [x, True], (tf.uint8,)\n",
    "# ), INPUT_SHAPE)/255, tf.cast(y, tf.uint8)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# dataset_storage[col_identifier][\"val_ds\"] = dataset_storage[col_identifier][\"val_ds\"].batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# dataset_storage[col_identifier][\"test_ds\"] = dataset_storage[col_identifier][\"raw_test_ds\"].map(lambda x,y: (tf.reshape(tf.py_function(\n",
    "#     load_npz, [x, True], (tf.uint8,)\n",
    "# ), INPUT_SHAPE)/255, tf.cast(y, tf.int32)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# dataset_storage[col_identifier][\"test_ds\"] = dataset_storage[col_identifier][\"test_ds\"].batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "INPUT_SHAPE = (128,128,32,1)\n",
    "INPUT_SHAPE[-1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('3.9.7': pyenv)"
  },
  "interpreter": {
   "hash": "547c98e2d128f0d954a25014c53f40094ed93b4078667c378730fcf6748ca4ce"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}